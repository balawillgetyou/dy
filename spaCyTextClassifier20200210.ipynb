{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCyTextClassifier20200210.ipynb",
      "provenance": [],
      "mount_file_id": "1HmxHEoCtYRxgdUxd-r78bG-3txI43uwb",
      "authorship_tag": "ABX9TyPBL8vGzN9TZT1xqNk+SJB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balawillgetyou/dy/blob/master/spaCyTextClassifier20200210.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3f3KAnPbZZT",
        "colab_type": "text"
      },
      "source": [
        "# Objective:\n",
        "To demonstrate a Hierarchical Attention Network text classifer, built using spaCy. The data is from a hackathon where the author needs to be identified, from a paragraph of text. spaCy's default hyper parameter settings work well. To illustrate tuning these, a decaying drop out is also demonstrated and proves the point that hyper parameter tuning is extremely hard.  \n",
        "The network is trained for just 5 iterations for this demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaymOp5_1AYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#library needed for text classifier training\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding, decaying\n",
        "\n",
        "#library to load and process data\n",
        "import os\n",
        "import copy\n",
        "import re\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) #to prevent cell display truncation\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from __future__ import unicode_literals\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "#to save trained model\n",
        "#from pathlib import Path\n",
        "\n",
        "#metrics calculation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "from sklearn.utils import resample#downsample not-an-obligation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xejd-RO51-yB",
        "colab_type": "text"
      },
      "source": [
        "Instructions to load data from Google Drive. Use the \"Files\" hamburger icon on the top left. Mount Drive. Ensure \"drive\" is visible in the folders icon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWucQGpv-XAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_string(mystring):\n",
        "    return re.sub('[^A-Za-z\\ 0-9 ]+', '', mystring)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRuv94Be2WNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadPreProcData():\n",
        "    \n",
        "    temp_1 = open(\"/content/drive/My Drive/Colab_Notebooks_Bala/WhoseLineIsItAnywayTRAIN.csv\", 'r', encoding='latin-1') \n",
        "    WhoseLineData = pd.read_csv(temp_1)\n",
        "    WhoseLineData = shuffle(WhoseLineData)\n",
        "    WhoseLineData = WhoseLineData.iloc[0:4999,]\n",
        "\n",
        "    #data split to execute validations using unseen data\n",
        "    df_train, df_val = train_test_split(WhoseLineData, test_size=0.15)\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "    df_val = df_val.reset_index(drop=True)\n",
        "    #df_val.to_csv(\"/content/drive/My Drive/Colab_Notebooks_Bala/trainedModel/WhoseLine_val.csv\")\n",
        "\n",
        "    #conversion from dataframe to a list of nested dictionaries that look like this:\n",
        "    #('The grey donkey paid no attention to thisspeech because he had just stopped before a house which had painted overthe doorway a pair of hoofs with a donkey tail between them and a rudecrown and sceptre aboveIll see if his magnificent Majesty King Kikabray is at home saidhe He lifted his head and called Wheehaw wheehaw wheehaw threetimes in a shocking voice turning about and kicking with his heelsagainst the panel of the door For a time there was no reply then thedoor opened far enough to permit a donkeys head to stick out and lookat themIt was a white head with big awful ears and round solemn eyesHave the foxes gone it asked in a trembling voiceThey havent been here most stupendous Majesty replied the grey oneThe new arrivals prove to be travelers of distinctionOh said the King in a relieved tone of voice Let them come inHe opened the door wide and the party marched into a big room whichDorothy thought looked quite unlike a kings palace There were mats ofwoven grasses on the floor and the place was clean and neat but hisMajesty had no other furniture at allperhaps because he didnt needit He squatted down in the center of the room and a little brown donkeyran and brought a big gold crown which it placed on the monarchs headand a golden staff with a jeweled ball at the end of it which the Kingheld between his front hoofs as he sat uprightNow then said his Majesty waving his long ears gently to and frotell me why you are here and what you expect me to do for you Heeyed ButtonBright rather sharply as if afraid of the little boysqueer head though it was the shaggy man who undertook to replyMost noble and supreme ruler of Dunkiton he said trying not to laughin the solemn Kings face we are strangers traveling through yourdominions and have entered your magnificent city because the road ledthrough it and there was no way to go around All we desire is to payour respects to your Majestythe cleverest king in all the world Imsureand then to continue on our wayThis polite speech pleased the King very much indeed it pleased him somuch that it proved an unlucky speech for the shaggy man Perhaps theLove Magnet helped to win his Majestys affection as well as theflattery but however this may be the white donkey looked kindly uponthe speaker and saidOnly a donkey should be able to use such fine big words and you aretoo wise and admirable in all ways to be a mere man Also I feel that Ilove you as well as I do my own favored people so I will bestow uponyou the greatest gift within my powera donkeys headAs he spoke he waved his jeweled staff Although the shaggy man criedout and tried to leap backward and escape it proved of no use Suddenlyhis own head was gone and a donkey head appeared in its placea brownshaggy head so absurd and droll that Dorothy and Polly both broke intomerry laughter and even ButtonBrights fox face wore a smileDear me dear me cried the shaggy man feeling of his shaggy new headand his long ears What a misfortunewhat a great misfortune Give meback my own head you stupid kingif you love me at allDont you like it asked the King surprisedHeehaw I hate it Take it awayquick said the shaggy manKING KICKABRAY WORKS MAGIC ON THE SHAGGY MANBut I cant do that was the reply My magic works only one way',\n",
        "    #{'cats': {0: 0, 1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}})\n",
        "    \n",
        "    author = WhoseLineData['author'].unique()\n",
        "    labels_default = dict((v, 0) for v in author)\n",
        "\n",
        "    train_data = []\n",
        "    for i, row in df_train.iterrows():\n",
        "\n",
        "        label_values = copy.deepcopy(labels_default)\n",
        "        label_values[row['author']] = 1\n",
        "        train_data.append((str(clean_string(row['text'])), {\"cats\": label_values}))\n",
        "    \n",
        "    return train_data, df_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyVDFTQ5LnxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this function uses default spaCy hyperparameter settings, including the optimizer, minibatch size and drop out\n",
        "def trainSpacy(model=None, output_dir=None, n_iter=5):\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank('en')  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # add the text classifier to the pipeline if it doesn't exist\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'textcat' not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe('textcat')\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        textcat = nlp.get_pipe('textcat')\n",
        "\n",
        "    # add label to text classifier\n",
        "    for i in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        textcat.add_label(i)\n",
        "\n",
        "    train_data, df_val = loadPreProcData()\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        optimizer = nlp.begin_training()\n",
        "        print(\"Training the model...\")\n",
        "        print('{:^5}\\t'.format('LOSS'))\n",
        "        for i in range(n_iter):\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.25,losses=losses)\n",
        "            \n",
        "            print('{0:.3f}'  # print a simple table\n",
        "                  .format(losses['textcat']))\n",
        "    return [nlp, df_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m4WETiCTN_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this function demonstrates drop out decay\n",
        "def trainSpacyDropOut(model=None, output_dir=None, n_iter=5):\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank('en')  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # add the text classifier to the pipeline if it doesn't exist\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'textcat' not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe('textcat')\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        textcat = nlp.get_pipe('textcat')\n",
        "\n",
        "    # add label to text classifier\n",
        "    for i in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        textcat.add_label(i)\n",
        "\n",
        "    train_data, df_val = loadPreProcData()\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        optimizer = nlp.begin_training()\n",
        "        print(\"Training the model...\")\n",
        "        print('{:^5}\\t'.format('LOSS'))\n",
        "        for i in range(n_iter):\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
        "            dropout = decaying(0.6, 0.2, 1e-4)\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=next(dropout),losses=losses)\n",
        "            \n",
        "            print('{0:.3f}'  # print a simple table\n",
        "                  .format(losses['textcat']))\n",
        "    return [nlp, df_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOPqUgTVNuLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictObligationType(nlp, df):\n",
        "    predictedClass = []\n",
        "    texts = df['text']\n",
        "    \n",
        "    for aa in range(len(df)):\n",
        "        test_text = texts.iloc[aa]\n",
        "        doc = nlp(test_text)\n",
        "        lis = doc.cats.items()\n",
        "        predictedClass.append(max(lis, key=itemgetter(1))[0])\n",
        "        \n",
        "    df['Prediction'] = pd.DataFrame(predictedClass)\n",
        "\n",
        " \n",
        "    return(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqwVNfQGOX4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoreIt(nlp, df):\n",
        "\n",
        "    predictions = predictObligationType(nlp, df)\n",
        "    \n",
        "    actualClass = df['author']\n",
        "    predictedClass = df['Prediction']\n",
        "\n",
        "    precision, recall, fscore, support = score(actualClass, predictedClass, labels = [0,1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "    results1 = pd.concat((pd.Series([0,1,2,3,4,5,6,7,8,9]), pd.Series(precision), pd.Series(recall), pd.Series(fscore), pd.Series(support)), axis=1, join = 'outer')\n",
        "    results1.columns=('label','precision', 'recall', 'fscore', 'support')\n",
        "    print('spaCy','\\n','*'*50,'\\n',round(results1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWLyoAQedjCa",
        "colab_type": "text"
      },
      "source": [
        "First we train the classifier with default settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6aw4vI3MD0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b4411816-68af-45ea-9df5-093e3a9beeec"
      },
      "source": [
        "nlp, df_val = trainSpacy()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Training the model...\n",
            "LOSS \t\n",
            "26.309\n",
            "13.997\n",
            "8.423\n",
            "5.459\n",
            "3.408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFs09t9cP5_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "17803c2a-b4e7-4ca5-d241-897120cbed35"
      },
      "source": [
        "scoreIt(nlp, df_val)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy \n",
            " ************************************************** \n",
            "    label  precision  recall  fscore  support\n",
            "0  0      0.48       0.91    0.63    133    \n",
            "1  1      0.52       0.68    0.59    22     \n",
            "2  2      1.00       0.72    0.84    118    \n",
            "3  3      0.96       0.92    0.94    51     \n",
            "4  4      0.73       0.98    0.83    128    \n",
            "5  5      0.86       0.31    0.45    144    \n",
            "6  6      0.72       0.76    0.74    37     \n",
            "7  7      0.97       0.67    0.79    42     \n",
            "8  8      0.86       0.71    0.78    35     \n",
            "9  9      1.00       0.35    0.52    40     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYl9pUmod8Vb",
        "colab_type": "text"
      },
      "source": [
        "Next we train the classifier with updated hyper parameters. Hard to see any improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnn5k9qIWZU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "23f81c43-4900-46df-8660-7252edaaa6e3"
      },
      "source": [
        "nlpDropOut, df_valDropOut = trainSpacyDropOut()\n",
        "scoreIt(nlpDropOut, df_valDropOut)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Training the model...\n",
            "LOSS \t\n",
            "35.087\n",
            "21.581\n",
            "16.405\n",
            "13.397\n",
            "11.527\n",
            "spaCy \n",
            " ************************************************** \n",
            "    label  precision  recall  fscore  support\n",
            "0  0      0.98       0.76    0.85    135    \n",
            "1  1      0.63       0.57    0.60    30     \n",
            "2  2      0.50       0.95    0.66    107    \n",
            "3  3      0.65       0.96    0.78    53     \n",
            "4  4      0.64       0.92    0.75    145    \n",
            "5  5      0.97       0.27    0.42    131    \n",
            "6  6      0.33       0.07    0.11    30     \n",
            "7  7      0.84       0.55    0.67    47     \n",
            "8  8      0.00       0.00    0.00    32     \n",
            "9  9      0.23       0.32    0.27    40     \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}